{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Learn Spark?\n",
    "Spark is currently one of the most popular tools for big data analytics. You might have heard of other tools such as Hadoop. Hadoop is a slightly older technology although still in use by some companies. Spark is generally faster than Hadoop, which is why Spark has become more popular over the last few years.\n",
    "\n",
    "There are many other big data tools and systems, each with its own use case. For example, there are database system like **[Apache Cassandra](http://cassandra.apache.org/)** and SQL query engines like **[Presto](https://prestodb.io/)**. But Spark is still one of the most popular tools for analyzing large data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUIZ QUESTION\n",
    "To test your current hardware knowledge, match each computer hardware component with the best corresponding description. Don't worry if you're not sure. You'll get more information about this in the next few videos.\n",
    "\n",
    "|HARDWARE COMPONENT|DESCRIPTION|\n",
    "|------------------|-----------|\n",
    "|Memory            |Short-term, quick data storage|\n",
    "|Solid State Drive |Long-term, sage data storage|\n",
    "|Network|Connection between computers|\n",
    "|CPU|Brain of the computer|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numers Everyone Should Know\n",
    "### CPU (Central Processing Unit)\n",
    "The CPU is the \"brain\" of the computer. Every process on your computer is eventually handled by your CPU. This includes calculations and also instructions for the other components of the compute.\n",
    "\n",
    "### Memory (RAM)\n",
    "When your program runs, data gets temporarily stored in memory before getting sent to the CPU. Memory is ephemeral storage - when your computer shuts down, the data in the memory is lost.\n",
    "\n",
    "### Storage (SSD or Magnetic Disk)\n",
    "Storage is used for keeping data over long periods of time. When a program runs, the CPU will direct the memory to temporarily load data from long-term storage.\n",
    "\n",
    "### Network (LAN or the Internet)\n",
    "Network is the gateway for anything that you need that isn't stored on your computer. The network could connect to other computers in the same room (a Local Area Network) or to a computer on the other side of the world, connected over the internet.\n",
    "\n",
    "### Other Numbers to Know?\n",
    "You may have noticed a few other numbers involving the L1 and L2 Cache, mutex locking, and branch mispredicts. While these concepts are important for a detailed understanding of what's going on inside your computer, you don't need to worry about them for this course. If you're curious to learn more, check out [Peter Norvig's original blog post](http://norvig.com/21-days.html) from a few years ago, and [an interactive version](http://people.eecs.berkeley.edu/~rcs/research/interactive_latency.html) for today's current hardware.\n",
    "\n",
    "<img src=\"images/four_key_machine_components.png\">\n",
    "\n",
    "### QUIZ QUESTION\n",
    "Rank the hardware component in order from fastest to slowest\n",
    "\n",
    "|SPEED|HARDWARE COMPONENT|\n",
    "|-----|------------------|\n",
    "|Fastest|CPU|\n",
    "|2nd Fastest|Memory(RAM)|\n",
    "|3rd Fastest|Disk Storage(SSD)|\n",
    "|Slowest|Network|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardware: CPU\n",
    "The CPU is the brains of a computer. The CPU has a few different functions including directing other components of a computer as well as running mathematical calculations. The CPU can also store small amounts of data inside itself in what are called **registers**. These registers hold data that the CPU is working with at the moment.\n",
    "\n",
    "For example, say you write a program that reads in a 40 MB data file and then analyzes the file. When you execute the code, the instructions are loaded into the CPU. The CPU then instructs the computer to take the 40 MB from disk and store the data in memory (RAM). If you want to sum a column of data, then the CPU will essentially take two numbers at a time and sum them together. The accumulation of the sum needs to be stored somewhere while the CPU grabs the next number.\n",
    "\n",
    "This cumulative sum will be stored in a register. The registers make computations more efficient: the registers avoid having to send data unnecessarily back and forth between memory (RAM) and the CPU.\n",
    "\n",
    "### QUESTION 1 OF 2\n",
    "A 2.5 Gigahertz CPU means that the CPU processes 2.5 billion operations per second. Let's say that for each operation, the CPU processes 8 bytes of data. How many bytes could this CPU process per second?\n",
    "- [ ] 312.5 million bytes per second\n",
    "- [ ] 3.2 billion bytes per second\n",
    "- [x] 20 billion bytes per second\n",
    "- [ ] I'm not sure how to calculate this\n",
    "\n",
    "### QUESTION 2 OF 2\n",
    "Twitter generates about 6,000 tweets per second, and each tweet contains 200 bytes. So in one day, Twitter generates data on the order of:\n",
    "\n",
    "(6000 tweets / second) x (86400 seconds / day) x (200 bytes / tweet) = 104 billion bytes / day\n",
    "\n",
    "Knowing that tweets create approximately 104 billion bytes of data per day, how long would it take the 2.5 GigaHertz CPU to analyze a full day of tweets?\n",
    "- [ ] 0.19 seconds\n",
    "- [ ] 3.5 seconds\n",
    "- [x] 5.2 seconds\n",
    "- [ ] 47 seconds\n",
    "- [ ] 136 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardware: Memory\n",
    "Most of the time CPU is sitting idle as it waits for the input data from the memory. Memory takes about 250 times longer to find a random byte than to process the same byte in the CPU. The solution is data is loaded sequentially. \n",
    "\n",
    "With CPU and Memory, what else do we need?\n",
    "It seems like the right combination of CPU and memory can help you quickly load and process data. We could build a single computer with lots of CPUs and a ton of memory. The computer would be incredibly fast.\n",
    "\n",
    "### QUESTION 1 OF 2\n",
    "What are the potential trade offs of creating one computer with a lots of CPUs and memory?\n",
    "\n",
    "It's true that you could build a computer with a ton of CPU and memory, which is effectively a supercomputer but it'll not be cost effective.\n",
    "\n",
    "### Characteristic of Memory\n",
    "* Efficient\n",
    "* Ephemeral\n",
    "* Expensive\n",
    "\n",
    "Beyond the fact that memory is expensive and ephemeral, we'll learn that for most use cases in the industry, memory and CPU aren't the bottleneck. Instead the storage and network, which you'll learn about in the next videos, slow down many tasks you'll work on in the industry.\n",
    "\n",
    "### QUESTION 2 OF 2\n",
    "What are the limitations of memory (RAM)?\n",
    "\n",
    "- [x] RAM is relatively expensive\n",
    "- [ ] You can only fit 8GB RAM onto a single computer\n",
    "- [x] Data stored in RAM gets erased when the computer shuts down\n",
    "- [ ] Operations in RAM are relatively inefficient compared to disk storage and network operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardware: Storage\n",
    "Long term storage like hard drive disk(magnetic disk) is cheap and durable but it is much slower than memory. Loading data from the hard drive can be 200 times slower. Even the new SSDs  are still about 15 times slower.\n",
    "\n",
    "<img src=\"images/diff_mem_storage.png\">\n",
    "\n",
    "This difference of seconds and mili seconds may seem small but when we are working with GBs and TBs of data then this difference adds up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardware: Network\n",
    "In the past few decades every part of the computer is improved doubling in efficiency every few years. Unfortunately the speed of our network lags behind the improvements in CPU, memory and Storage as demonstrated in the following figure:\n",
    "\n",
    "<img src=\"images/hardware_improvement.png\">\n",
    "\n",
    "As a result moving data from one machine to another is the most common bottleneck when working with big data.\n",
    "For example the same hour of tweets that would take half a second to process from storage would take 30 seconds to download from a twitter API on  a typical network.\n",
    "<img src=\"images/ssd_tweet.png\">\n",
    "\n",
    "<img src=\"images/network_tweet.png\">\n",
    "\n",
    "It usually takes at lest 20 times longer to prcess data when we have to download it from the other machine first. For this reason, the distributed systems try to minimize **Shuffling** the data back and forth.\n",
    "**Shuffling** is the process of moving data back and forth between nodes of a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardware: Key Ratios\n",
    "\n",
    "<img src=\"images/key_ratios.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small Data Numbers\n",
    "If you are working on a data that can fit into your memory then you are working on small data. Let's suppose you want to analyze 4 GB of log file and it can easily fit into the memory so it's the small data. Hence its better to use just a python code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Data Numbers\n",
    "Now suppose your log data explodes to 200 GB. The same python code won't work as you'll run out of the memory.\n",
    "\n",
    "### QUIZ QUESTION\n",
    "What happened to your computer when you started to run the program with the larger data set? Keep in mind that in this current configuration, the data set is stored on your local computer and does not need to travel across a network (Select all that apply)\n",
    "- [ ] It took too long to move data across the network\n",
    "- [x] The CPU couldn't load data quickly from memory\n",
    "- [x] The memory couldn't load data quickly from storage\n",
    "- [ ] The CPU simply couldn't handle the large data set\n",
    "- [ ] The storage couldn't load data quickly from the network\n",
    "- [ ] The CPU couldn't load data quickly from the network\n",
    "\n",
    "Click [here](https://www.youtube.com/watch?v=QjPr7qeJTQk) for more explanation about the question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Medium Data Numbers\n",
    "If a dataset is larger than the size of your RAM let's suppose 10 GB of data, you might still be able to analyze the data on a single computer. By default, the Python pandas library will read in an entire dataset from disk into memory. If the dataset is larger than your computer's memory, the program won't work.\n",
    "\n",
    "However, the Python pandas library can read in a file in smaller chunks. Thus, if you were going to calculate summary statistics about the dataset such as a sum or count, you could read in a part of the dataset at a time and accumulate the sum or count.\n",
    "\n",
    "[Here](http://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#io-chunking) is an example of how this works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## History of Distributed and Parallel Computing\n",
    "Distributed computing refers to the computer networks where individual computers are physically distributed within some geographical area and communicate with each other via message passing.\n",
    "\n",
    "The difference between distributed and parallel processing is that in parallel processing CPUs share the memory while in distributed computing each CPU has dedicated memory assigned to it.\n",
    "\n",
    "<img src=\"images/distributed_parallel.png\">\n",
    "\n",
    "### QUIZ QUESTION\n",
    "Mark the statements that are true.\n",
    "- [ ] Generally speaking, distributed computing assumes that multiple CPUs are sharing a single source of memory.\n",
    "- [ ] Parallel computing is another way of saying distributed computing\n",
    "- [x] In general parallel computing implies multiple CPUs share the same memory.\n",
    "- [x] With distributed computing, each CPU has its own memory.\n",
    "- [x] In distributed computing, each computer/machine is connected to the other machines across a network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Hadoop Ecosystem\n",
    "\n",
    "### Hadoop Vocabulary\n",
    "Here is a list of some terms associated with Hadoop. You'll learn more about these terms and how they relate to Spark in the rest of the lesson.\n",
    "\n",
    "* **[Hadoop](https://hadoop.apache.org/)** - an ecosystem of tools for big data storage and data analysis. Hadoop is an older system than Spark but is still used by many companies. The major difference between Spark and Hadoop is how they use memory. Hadoop writes intermediate results to disk whereas Spark tries to keep data in memory whenever possible. This makes Spark faster for many use cases.\n",
    "\n",
    "* **Hadoop MapReduce** - a system for processing and analyzing large data sets in parallel.\n",
    "\n",
    "* **Hadoop YARN** - a resource manager that schedules jobs across a cluster. The manager keeps track of what computer resources are available and then assigns those resources to specific tasks.\n",
    "\n",
    "* **Hadoop Distributed File System (HDFS)** - a big data storage system that splits data into chunks and stores the chunks across a cluster of computers.\n",
    "\n",
    "As Hadoop matured, other tools were developed to make Hadoop easier to work with. These tools included:\n",
    "\n",
    "* **Apache Pig** - a SQL-like language that runs on top of Hadoop MapReduce\n",
    "* **Apache Hive** - another SQL-like interface that runs on top of Hadoop MapReduce\n",
    "Oftentimes when someone is talking about Hadoop in general terms, they are actually talking about Hadoop MapReduce. However, Hadoop is more than just MapReduce. In the next part of the lesson, you'll learn more about how MapReduce works.\n",
    "\n",
    "### How is Spark related to Hadoop?\n",
    "Spark, which is the main focus of this course, is another big data framework. Spark contains libraries for data analysis, machine learning, graph analysis, and streaming live data. Spark is generally faster than Hadoop. This is because Hadoop writes intermediate results to disk whereas Spark tries to keep intermediate results in memory whenever possible.\n",
    "\n",
    "The Hadoop ecosystem includes a distributed file storage system called HDFS (Hadoop Distributed File System). Spark, on the other hand, does not include a file storage system. You can use Spark on top of HDFS but you do not have to. Spark can read in data from other sources as well such as **[Amazon S3](https://aws.amazon.com/s3/)**.\n",
    "\n",
    "### Streaming Data\n",
    "Data streaming is a specialized topic in big data. The use case is when you want to store and analyze data in real-time such as Facebook posts or Twitter tweets.\n",
    "\n",
    "Spark has a streaming library called **[Spark Streaming](https://spark.apache.org/docs/latest/streaming-programming-guide.html)** although it is not as popular and fast as some other streaming libraries. Other popular streaming libraries include **[Storm](http://storm.apache.org/)** and **[Flink](https://flink.apache.org/)**. Streaming won't be covered in this course, but you can follow these links to learn more about these technologies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MapReduce\n",
    "MapReduce is a programming technique for manipulating large data sets. \"Hadoop MapReduce\" is a specific implementation of this programming technique.\n",
    "\n",
    "The technique works by first dividing up a large dataset and distributing the data across a cluster. In the map step, each data is analyzed and converted into a (key, value) pair. Then these key-value pairs are shuffled across the cluster so that all keys are on the same machine. In the reduce step, the values with the same keys are combined together.\n",
    "\n",
    "While Spark doesn't implement MapReduce, you can write Spark programs that behave in a similar way to the map-reduce paradigm. Click [here](https://www.youtube.com/watch?v=ErgNIy7z4SE) to see the demonstration of how data is shuffled.\n",
    "\n",
    "### QUIZ QUESTION\n",
    "In the map-reduce paradigm, what happens in the shuffle step?\n",
    "- [ ] The data gets randomly shuffled for cross validation purposes\n",
    "- [ ] Calculations are done to find the sum of all values with the same key\n",
    "- [ ] Each value in the data set goes through some mathematical operation\n",
    "- [x] Data points with the same key get moved to the same cluster node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop MapReduce [Demo]\n",
    "### MapReduce\n",
    "\n",
    "The MapReduce programming technique was designed to analyze massive data sets across a cluster. In this Jupyter notebook, you'll get a sense for how Hadoop MapReduce works; however, this notebook will run locally rather than on a cluster.\n",
    "\n",
    "The biggest difference between Hadoop and Spark is that Spark tries to do as many calculations as possible in memory, which avoids moving data back and forth across a cluster. Hadoop writes intermediate calculations out to disk, which can be less efficient. Hadoop is an older technology than Spark and one of the cornerstone big data technologies.\n",
    "\n",
    "If you click on the Jupyter notebook logo at the top of the workspace, you'll be taken to the workspace directory. There you will see a file called \"songplays.txt\". This is a text file where each line represents a song that was played in the Sparkify app. The MapReduce code will count how many times each song was played. In other words, the code counts how many times the song title appears in the list.\n",
    "\n",
    "\n",
    "### MapReduce versus Hadoop MapReduce\n",
    "\n",
    "Don't get confused by the terminology! MapReduce is a programming technique. Hadoop MapReduce is a specific implementation of the programming technique.\n",
    "\n",
    "Some of the syntax will look a bit funny, so be sure to read the explanation and comments for each section. You'll learn more about the syntax in later lessons. \n",
    "\n",
    "Run each of the code cells below to see the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mrjob in c:\\users\\naqee\\anaconda3\\lib\\site-packages (0.7.1)\n",
      "Requirement already satisfied: PyYAML>=3.10 in c:\\users\\naqee\\anaconda3\\lib\\site-packages (from mrjob) (5.1.2)\n"
     ]
    }
   ],
   "source": [
    "# Install mrjob library. This package is for running MapReduce jobs with Python\n",
    "# In Jupyter notebooks, \"!\" runs terminal commands from inside notebooks \n",
    "\n",
    "! pip install mrjob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wordcount.py\n"
     ]
    }
   ],
   "source": [
    "%%file wordcount.py\n",
    "# %%file is an Ipython magic function that saves the code cell as a file\n",
    "\n",
    "from mrjob.job import MRJob # import the mrjob library\n",
    "\n",
    "class MRSongCount(MRJob):\n",
    "    \n",
    "    # the map step: each line in the txt file is read as a key, value pair\n",
    "    # in this case, each line in the txt file only contains a value but no key\n",
    "    # _ means that in this case, there is no key for each line\n",
    "    def mapper(self, _, song):\n",
    "        # output each line as a tuple of (song_names, 1) \n",
    "        yield (song, 1)\n",
    "\n",
    "    # the reduce step: combine all tuples with the same key\n",
    "    # in this case, the key is the song name\n",
    "    # then sum all the values of the tuple, which will give the total song plays\n",
    "    def reducer(self, key, values):\n",
    "        yield (key, sum(values))\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    MRSongCount.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Broken Networks\"\t510\n",
      "\"Data House Rock\"\t828\n",
      "\"Deep Dreams\"\t1131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory C:\\Users\\naqee\\AppData\\Local\\Temp\\wordcount.naqee.20200101.204731.275734\n",
      "Running step 1 of 1...\n",
      "job output is in C:\\Users\\naqee\\AppData\\Local\\Temp\\wordcount.naqee.20200101.204731.275734\\output\n",
      "Streaming final output from C:\\Users\\naqee\\AppData\\Local\\Temp\\wordcount.naqee.20200101.204731.275734\\output...\n",
      "Removing temp directory C:\\Users\\naqee\\AppData\\Local\\Temp\\wordcount.naqee.20200101.204731.275734...\n"
     ]
    }
   ],
   "source": [
    "# run the code as a terminal command\n",
    "! python wordcount.py data/songplays.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of what happens in the code.\n",
    "\n",
    "There is a list of songs in songplays.txt that looks like the following:\n",
    "\n",
    "Deep Dreams\n",
    "Data House Rock\n",
    "Deep Dreams\n",
    "Data House Rock\n",
    "Broken Networks\n",
    "Data House Rock\n",
    "etc.....\n",
    "\n",
    "During the map step, the code reads in the txt file one line at a time. The map steps outputs a set of tuples that look like this:\n",
    "\n",
    "(Deep Dreams, 1)  \n",
    "(Data House Rock, 1)  \n",
    "(Deep Dreams, 1)  \n",
    "(Data House Rock, 1)  \n",
    "(Broken Networks, 1)  \n",
    "(Data House Rock, 1)  \n",
    "etc.....\n",
    "\n",
    "Finally, the reduce step combines all of the values by keys and sums the values:  \n",
    "\n",
    "(Deep Dreams, \\[1, 1, 1, 1, 1, 1, ... \\])  \n",
    "(Data House Rock, \\[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\\])  \n",
    "(Broken Networks, \\[1, 1, 1, ...\\]  \n",
    "\n",
    "With the output \n",
    "\n",
    "(Deep Dreams, 1131)  \n",
    "(Data House Rock, 510)  \n",
    "(Broken Networks, 828)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Spark Cluster\n",
    "When we talk about distributed computing it means a big computational job executed across a cluster of nodes. Each node is responsible for a set of operations on a subset of the data. At the end we combine these partial results to get the final answer. Most computational frameworks are organized into master slave hierarchy. Where the master node is responsible for orchestrating the tasks across the cluster, while the slaves are performing the actual computations. \n",
    "\n",
    "<img src=\"images/spark-modes.png\">\n",
    "\n",
    "### QUIZ QUESTION\n",
    "In which cases would you use Local Mode?\n",
    "- [ ] When you are working with a computer cluster that only has two machines \n",
    "- [ ] For distributed computing across a cluster\n",
    "- [x] When you are working with Spark installed on your own laptop\n",
    "- [ ] THere is never reason to use local mode since you're not taking advantage of Spark's ability to work on a cluster: you should be using Standalone, YARN or Mesos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Use Cases and Resources\n",
    "Here are a few resources about different Spark use cases:\n",
    "\n",
    "* **[Data Analytics](http://spark.apache.org/sql/)**\n",
    "* **[Machine Learning](http://spark.apache.org/mllib/)**\n",
    "* **[Streaming](http://spark.apache.org/streaming/)**\n",
    "* **[Graph Analytics](http://spark.apache.org/graphx/)**\n",
    "\n",
    "### You Don't Always Need Spark\n",
    "Spark is meant for big data sets that cannot fit on one computer. But you don't need Spark if you are working on smaller data sets. In the cases of data sets that can fit on your local computer, there are many other options out there you can use to manipulate data such as:\n",
    "\n",
    "* **[AWK](https://en.wikipedia.org/wiki/AWK)** - a command line tool for manipulating text files\n",
    "* **[R](https://www.r-project.org/)** - a programming language and software environment for statistical computing\n",
    "* **[Python PyData Stack](https://pydata.org/downloads.html)**, which includes pandas, Matplotlib, NumPy, and scikit-learn among other libraries\n",
    "Sometimes, you can still use pandas on a single, local machine even if your data set is only a little bit larger than memory. Pandas can read data in chunks. Depending on your use case, you can filter the data and write out the relevant parts to disk.\n",
    "\n",
    "If the data is already stored in a relational database such as **[MySQL](https://www.mysql.com/)** or **[Postgres](https://www.postgresql.org/)**, you can leverage SQL to extract, filter and aggregate the data. If you would like to leverage pandas and SQL simultaneously, you can use libraries such as **[SQLAlchemy](https://www.sqlalchemy.org/)**, which provides an abstraction layer to manipulate SQL tables with generative Python expressions.\n",
    "\n",
    "The most commonly used Python Machine Learning library is **[scikit-learn](http://scikit-learn.org/stable/)**. It has a wide range of algorithms for classification, regression, and clustering, as well as utilities for preprocessing data, fine tuning model parameters and testing their results. However, if you want to use more complex algorithms - like deep learning - you'll need to look further. **[TensorFlow](https://www.tensorflow.org/)** and **[PyTorch](https://pytorch.org/)** are currently popular packages.\n",
    "\n",
    "### Spark's Limitations\n",
    "Spark has some limitation.\n",
    "\n",
    "Spark Streamingâ€™s latency is at least 500 milliseconds since it operates on micro-batches of records, instead of processing one record at a time. Native streaming tools such as **[Storm](http://storm.apache.org/)**,**[Apex](https://apex.apache.org/)**, or **[Flink](https://flink.apache.org/)** can push down this latency value and might be more suitable for low-latency applications. Flink and Apex can be used for batch computation as well, so if you're already using them for stream processing, there's no need to add Spark to your stack of technologies.\n",
    "\n",
    "Another limitation of Spark is its selection of machine learning algorithms. Currently, Spark only supports algorithms that scale linearly with the input data size. In general, deep learning is not available either, though there are many projects integrate Spark with Tensorflow and other deep learning tools.\n",
    "\n",
    "### Hadoop versus Spark\n",
    "The Hadoop ecosystem is a slightly older technology than the Spark ecosystem. In general, Hadoop MapReduce is slower than Spark because Hadoop writes data out to disk during intermediate steps. However, many big companies, such as Facebook and LinkedIn, started using Big Data early and built their infrastructure around the Hadoop ecosystem.\n",
    "\n",
    "While Spark is great for iterative algorithms, there is not much of a performance boost over Hadoop MapReduce when doing simple counting. Migrating legacy code to Spark, especially on hundreds of nodes that are already in production, might not be worth the cost for the small performance boost.\n",
    "\n",
    "### Beyond Spark for Storing and Processing Big Data\n",
    "Keep in mind that Spark is not a data storage system, and there are a number of tools besides Spark that can be used to process and analyze large datasets.\n",
    "\n",
    "Sometimes it makes sense to use the power and simplicity of SQL on big data. For these cases, a new class of databases, know as NoSQL and NewSQL, have been developed.\n",
    "\n",
    "For example, you might hear about newer database storage systems like **[HBase](https://hbase.apache.org/)** or **[Cassandra](http://cassandra.apache.org/)**. There are also distributed SQL engines like **[Impala](https://impala.apache.org/)** and **[Presto](https://prestodb.io/)**. Many of these technologies use query syntax that you are likely already familiar with based on your experiences with Python and SQL.\n",
    "\n",
    "In the lessons ahead, you will learn about Spark specifically, but know that many of the skills you already have with SQL, Python, and soon enough, Spark, will also be useful if you end up needing to learn any of these additional Big Data tools."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
